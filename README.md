# ğŸ™ï¸ WhisperX Speaker Transcription Pipeline

> Transcribes audio and video into timestamped, speaker-labeled transcripts using WhisperX, Voice Activity Detection, alignment, and speaker diarization.

---

## âœ¨ Features

* ğŸ§ Supports audio **and** video input
* ğŸ§  Automatic speech recognition using WhisperX
* ğŸ—£ï¸ Multi-speaker diarization (Speaker A / Speaker B)
* â±ï¸ Precise timestamp alignment
* ğŸ“„ Generates both human-readable and structured outputs
* ğŸ“¦ Works with most media formats via FFmpeg
* ğŸªŸ Windows, WSL, and Linux compatible
* ğŸ” Uses HuggingFace authentication for diarization models

---

## ğŸ“ Output

For each input file:

```
example.mp4
```

The pipeline generates:

```
example.txt              â†’ readable transcript
example.json             â†’ structured speaker segments
transcription.log        â†’ processing log
```

---

## ğŸ“œ Example Output

### TXT

```
[12.4s - 18.1s] Speaker A: ADHD is not a deficit...
[18.2s - 23.6s] Speaker B: Thatâ€™s actually a huge misunderstanding...
```

### JSON

```json
{
  "speaker": "Speaker A",
  "start": 12.4,
  "end": 18.1,
  "text": "ADHD is not a deficit..."
}
```

---

## ğŸ¬ Supported Input Formats

### Audio

* mp3
* wav
* m4a
* flac
* ogg
* opus
* aac

### Video

* mp4
* mkv
* mov
* avi
* webm

---

## âš™ï¸ Requirements

* Python 3.10+
* FFmpeg
* HuggingFace account (for diarization models)

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/whisperx-transcriber.git
cd whisperx-transcriber
```

---

### 2ï¸âƒ£ Create Virtual Environment

#### Windows PowerShell

```powershell
py -3.10 -m venv venv
.\venv\Scripts\Activate.ps1
```

#### Windows CMD

```cmd
venv\Scripts\activate.bat
```

#### Linux / WSL

```bash
python3 -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -U whisperx python-dotenv omegaconf
```

---

### 4ï¸âƒ£ Install FFmpeg

#### Windows (Recommended)

```powershell
winget install ffmpeg
```

Verify installation:

```bash
ffmpeg -version
```

---

### 5ï¸âƒ£ Create `.env` File

```
HF_TOKEN=your_huggingface_token_here
MODEL_SIZE=base
```

Get a HuggingFace token here:

ğŸ‘‰ [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

---

## ğŸš€ Usage

```bash
python whisper_to_text_diarized.py "input_file.mp4"
```

---

## ğŸ§  Pipeline Overview

```
Media Input
   â†“
FFmpeg Conversion
   â†“
Voice Activity Detection (Silero)
   â†“
WhisperX Transcription
   â†“
Timestamp Alignment
   â†“
Speaker Diarization (Pyannote)
   â†“
TXT + JSON Output
```

---

## âš¡ Performance Notes

| Hardware | 30 Minute File |
| -------- | -------------- |
| CPU      | ~15-30 minutes |
| GPU      | ~5-10 minutes  |

---

## ğŸ§© Configuration Options

Inside `.env`:

```
MODEL_SIZE=base
FORCE_CPU=0
MIN_SPEAKERS=2
MAX_SPEAKERS=2
```

---

## ğŸ§ª Troubleshooting

### FFmpeg Not Found

Install FFmpeg and ensure it is available in your system PATH.

---

### HuggingFace Token Missing

Diarization requires authentication. Add your token to `.env`.

---

### PyTorch Safe Loading Errors

The script automatically allowlists trusted model checkpoint classes.

---

## ğŸ“¦ Dependencies

* WhisperX
* Pyannote Audio
* PyTorch
* FFmpeg
* Python-Dotenv

---

## ğŸ—ºï¸ Roadmap

* Batch folder transcription
* Speaker name training
* Transcript summarization
* Semantic transcript search
* NotebookLM formatting
* Real-time folder watcher

---

## ğŸ¤ Contributing

Pull requests are welcome. Feature ideas and optimizations are encouraged.

---

## ğŸ“œ License

MIT License

---

## â­ Acknowledgements

* OpenAI Whisper
* WhisperX
* Pyannote Audio
* Silero VAD
* FFmpeg

---

## ğŸ’¡ Future Vision

This project aims to provide a reliable foundation for:

* Podcast processing
* Research transcription
* AI knowledge ingestion
* Meeting automation
* Content indexing
